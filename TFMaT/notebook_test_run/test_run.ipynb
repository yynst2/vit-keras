{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3375112d-2113-4230-9a22-00421518759f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/faustomorales/vit-keras/releases/download/dl/ViT-B_16_imagenet21k+imagenet2012.npz\n",
      "347504640/347502902 [==============================] - 12s 0us/step\n",
      "347512832/347502902 [==============================] - 12s 0us/step\n",
      "Granny Smith\n"
     ]
    }
   ],
   "source": [
    "# !pip install opencv-python\n",
    "# !pip install vit-keras\n",
    "# !pip install tensorflow_addons\n",
    "\n",
    "from vit_keras import vit, utils\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "image_size = 384\n",
    "classes = utils.get_imagenet_classes()\n",
    "model = vit.vit_b16(\n",
    "    image_size=image_size,\n",
    "    activation='sigmoid',\n",
    "    pretrained=True,\n",
    "    include_top=True,\n",
    "    pretrained_top=True\n",
    ")\n",
    "url = 'https://upload.wikimedia.org/wikipedia/commons/d/d7/Granny_smith_and_cross_section.jpg'\n",
    "image = utils.read(url, image_size)\n",
    "X = vit.preprocess_inputs(image).reshape(1, image_size, image_size, 3)\n",
    "y = model.predict(X)\n",
    "print(classes[y[0].argmax()]) # Granny smith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ffeaf7a-50d7-4eb3-a956-4b4e446fb57b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c557c178-30d2-4dbe-9560-2fdfa356e4d4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vit-b16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 384, 384, 3)]     0         \n",
      "                                                                 \n",
      " embedding (Conv2D)          (None, 24, 24, 768)       590592    \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 576, 768)          0         \n",
      "                                                                 \n",
      " class_token (ClassToken)    (None, 577, 768)          768       \n",
      "                                                                 \n",
      " Transformer/posembed_input   (None, 577, 768)         443136    \n",
      " (AddPositionEmbs)                                               \n",
      "                                                                 \n",
      " Transformer/encoderblock_0   ((None, 577, 768),       7087872   \n",
      " (TransformerBlock)           (None, 12, None, None))            \n",
      "                                                                 \n",
      " Transformer/encoderblock_1   ((None, 577, 768),       7087872   \n",
      " (TransformerBlock)           (None, 12, None, None))            \n",
      "                                                                 \n",
      " Transformer/encoderblock_2   ((None, 577, 768),       7087872   \n",
      " (TransformerBlock)           (None, 12, None, None))            \n",
      "                                                                 \n",
      " Transformer/encoderblock_3   ((None, 577, 768),       7087872   \n",
      " (TransformerBlock)           (None, 12, None, None))            \n",
      "                                                                 \n",
      " Transformer/encoderblock_4   ((None, 577, 768),       7087872   \n",
      " (TransformerBlock)           (None, 12, None, None))            \n",
      "                                                                 \n",
      " Transformer/encoderblock_5   ((None, 577, 768),       7087872   \n",
      " (TransformerBlock)           (None, 12, None, None))            \n",
      "                                                                 \n",
      " Transformer/encoderblock_6   ((None, 577, 768),       7087872   \n",
      " (TransformerBlock)           (None, 12, None, None))            \n",
      "                                                                 \n",
      " Transformer/encoderblock_7   ((None, 577, 768),       7087872   \n",
      " (TransformerBlock)           (None, 12, None, None))            \n",
      "                                                                 \n",
      " Transformer/encoderblock_8   ((None, 577, 768),       7087872   \n",
      " (TransformerBlock)           (None, 12, None, None))            \n",
      "                                                                 \n",
      " Transformer/encoderblock_9   ((None, 577, 768),       7087872   \n",
      " (TransformerBlock)           (None, 12, None, None))            \n",
      "                                                                 \n",
      " Transformer/encoderblock_10  ((None, 577, 768),       7087872   \n",
      "  (TransformerBlock)          (None, 12, None, None))            \n",
      "                                                                 \n",
      " Transformer/encoderblock_11  ((None, 577, 768),       7087872   \n",
      "  (TransformerBlock)          (None, 12, None, None))            \n",
      "                                                                 \n",
      " Transformer/encoder_norm (L  (None, 577, 768)         1536      \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " ExtractToken (Lambda)       (None, 768)               0         \n",
      "                                                                 \n",
      " head (Dense)                (None, 1000)              769000    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 86,859,496\n",
      "Trainable params: 86,859,496\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model parameters\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da233bae-dcb8-4db5-b709-6351b1657e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model,to_file='model.pdf',show_shapes=True,show_dtype=True,expand_nested=True,show_layer_activations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed8dbb1-dd4c-433f-9b8e-f27c642658da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
